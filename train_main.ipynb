{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from utils import CocoImageFolder, coco_eval, to_var\n",
    "from data_loader import get_loader \n",
    "from adaptive import Encoder2Decoder\n",
    "from build_vocab import Vocabulary\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    # To reproduce training results\n",
    "    torch.manual_seed( args.seed )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed( args.seed )\n",
    "        \n",
    "    # Create model directory\n",
    "    if not os.path.exists( args.model_path ):\n",
    "        os.makedirs(args.model_path)\n",
    "    \n",
    "    # Image Preprocessing\n",
    "    # For normalization, see https://github.com/pytorch/vision#models\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.RandomCrop( args.crop_size ),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(( 0.485, 0.456, 0.406 ), \n",
    "                             ( 0.229, 0.224, 0.225 ))])\n",
    "    \n",
    "    # Load vocabulary wrapper.\n",
    "    with open( args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load( f )\n",
    "    \n",
    "    # Build training data loader\n",
    "    data_loader = get_loader( args.image_dir, args.caption_path, vocab, \n",
    "                              transform, args.batch_size,\n",
    "                              shuffle=True, num_workers=args.num_workers ) \n",
    "\n",
    "    # Load pretrained model or build from scratch\n",
    "    adaptive = Encoder2Decoder( args.embed_size, len(vocab), args.hidden_size )\n",
    "    \n",
    "    if args.pretrained:\n",
    "        \n",
    "        adaptive.load_state_dict( torch.load( args.pretrained ) )\n",
    "        # Get starting epoch #, note that model is named as '...your path to model/algoname-epoch#.pkl'\n",
    "        # A little messy here.\n",
    "        start_epoch = int( args.pretrained.split('/')[-1].split('-')[1].split('.')[0] ) + 1\n",
    "        \n",
    "    else:\n",
    "        start_epoch = 1\n",
    "    \n",
    "    # Constructing CNN parameters for optimization, only fine-tuning higher layers\n",
    "    cnn_subs = list( adaptive.encoder.resnet_conv.children() )[ args.fine_tune_start_layer: ]\n",
    "    cnn_params = [ list( sub_module.parameters() ) for sub_module in cnn_subs ]\n",
    "    cnn_params = [ item for sublist in cnn_params for item in sublist ]\n",
    "    \n",
    "    cnn_optimizer = torch.optim.Adam( cnn_params, lr=args.learning_rate_cnn, \n",
    "                                      betas=( args.alpha, args.beta ) )\n",
    "    \n",
    "    # Other parameter optimization\n",
    "    params = list( adaptive.encoder.affine_a.parameters() ) + list( adaptive.encoder.affine_b.parameters() ) \\\n",
    "                + list( adaptive.decoder.parameters() )\n",
    "\n",
    "    # Will decay later    \n",
    "    learning_rate = args.learning_rate\n",
    "    \n",
    "    # Language Modeling Loss, Optimizers\n",
    "    LMcriterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Change to GPU mode if available\n",
    "    if torch.cuda.is_available():\n",
    "        adaptive.cuda()\n",
    "        LMcriterion.cuda()\n",
    "    \n",
    "    # Train the Models\n",
    "    total_step = len( data_loader )\n",
    "    \n",
    "    cider_scores = []\n",
    "    best_cider = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Start Training \n",
    "    for epoch in range( start_epoch, args.num_epochs + 1 ):\n",
    "        \n",
    "        optimizer = torch.optim.Adam( params, lr=learning_rate )\n",
    "        \n",
    "        # Language Modeling Training\n",
    "        print ('------------------Training for Epoch %d----------------'%(epoch))\n",
    "        for i, (images, captions, lengths, _ ) in enumerate( data_loader ):\n",
    "\n",
    "            # Set mini-batch dataset\n",
    "            images = to_var( images )\n",
    "            captions = to_var( captions )\n",
    "            lengths = [ cap_len - 1  for cap_len in lengths ]\n",
    "            targets = pack_padded_sequence( captions[:,1:], lengths, batch_first=True )[0]\n",
    "\n",
    "            # Forward, Backward and Optimize\n",
    "            adaptive.train()\n",
    "            adaptive.zero_grad()\n",
    "\n",
    "            packed_scores = adaptive( images, captions, lengths )\n",
    "\n",
    "            # Compute loss and backprop\n",
    "            loss = LMcriterion( packed_scores[0], targets )\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for gradient exploding problem in LSTM\n",
    "            for p in adaptive.decoder.LSTM.parameters():\n",
    "                p.data.clamp_( -args.clip, args.clip )\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Start learning rate decay\n",
    "            if epoch > args.lr_decay:\n",
    "                \n",
    "                frac = ( epoch - args.cnn_epoch ) / args.learning_rate_decay_every\n",
    "                decay_factor = math.pow( 0.5, frac )\n",
    "\n",
    "                # Decay the learning rate\n",
    "                learning_rate = learning_rate * decay_factor\n",
    "            \n",
    "            # Start CNN fine-tuning\n",
    "            if epoch > args.cnn_epoch:\n",
    "\n",
    "                cnn_optimizer.step()\n",
    "\n",
    "            # Print log info\n",
    "            if i % args.log_step == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], CrossEntropy Loss: %.4f, Perplexity: %5.4f'%( epoch, \n",
    "                                                                                                 args.num_epochs, \n",
    "                                                                                                 i, total_step, \n",
    "                                                                                                 loss.data[0],\n",
    "                                                                                                 np.exp( loss.data[0] ) ))  \n",
    "                \n",
    "        # Save the Adaptive Attention model after each epoch\n",
    "        torch.save( adaptive.state_dict(), \n",
    "                   os.path.join( args.model_path, 'adaptive-%d.pkl'%( epoch ) ) )\n",
    "        \n",
    "        # Evaluation on validation set        \n",
    "        cider = coco_eval( adaptive, args, epoch )\n",
    "        cider_scores.append( cider )        \n",
    "        \n",
    "        if cider > best_cider:\n",
    "            best_cider = cider\n",
    "            best_epoch = epoch\n",
    "       \n",
    "        if len( cider_scores ) > 5:\n",
    "            \n",
    "            last_6 = cider_scores[-6:]\n",
    "            last_6_max = max( last_6 )\n",
    "            \n",
    "            # Test if there is improvement, if not do early stopping\n",
    "            if last_6_max != best_cider:\n",
    "                \n",
    "                print ('No improvement with CIDEr in the last 6 epochs...Early stopping triggered.')\n",
    "                print ('Model of best epoch #: %d with CIDEr score %.2f'%( best_epoch, best_cider ))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Model and Training Details--------------------------\n",
      "Namespace(alpha=0.8, batch_size=60, beta=0.999, caption_path='./data/annotations/captions_train2014.json', caption_val_path='./data/annotations/captions_val2014.json', clip=0.1, cnn_epoch=20, crop_size=224, embed_size=256, eval_size=30, f='/run/user/1003/jupyter/kernel-2ca3c78f-6bc8-48dd-a563-f61df1bcaaa9.json', fine_tune_start_layer=6, hidden_size=512, image_dir='./data/resized/train2014', learning_rate=0.0004, learning_rate_cnn=0.0001, learning_rate_decay_every=50, log_step=10, lr_decay=20, model_path='./models-attentive/', num_epochs=50, num_workers=2, pretrained='', seed=123, val_dir='./data/resized/val2014', vocab_path='./data/vocab.pkl')\n",
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n",
      "------------------Training for Epoch 1----------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/kydn8237872/AdaptiveAttention/data_loader.py\", line 39, in __getitem__\n    image = Image.open( os.path.join( self.root, path ) ).convert('RGB')\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/PIL/Image.py\", line 2543, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: './data/resized/train2014/COCO_train2014_000000342949.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f0930678ca67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-aaefa6b2f9fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Language Modeling Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'------------------Training for Epoch %d----------------'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Set mini-batch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/kydn8237872/AdaptiveAttention/data_loader.py\", line 39, in __getitem__\n    image = Image.open( os.path.join( self.root, path ) ).convert('RGB')\n  File \"/home/kydn8237872/pyenv35/lib/python3.5/site-packages/PIL/Image.py\", line 2543, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: './data/resized/train2014/COCO_train2014_000000342949.jpg'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Add attributes.')\n",
    "    parser.add_argument( '-f', default='self', help='To make it runnable in jupyter' )\n",
    "    parser.add_argument( '--model_path', type=str, default='./models-attentive/',\n",
    "                         help='path for saving trained models')\n",
    "    parser.add_argument('--crop_size', type=int, default=224 ,\n",
    "                        help='size for randomly cropping images')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl',\n",
    "                        help='path for vocabulary wrapper')\n",
    "    parser.add_argument('--image_dir', type=str, default='./data/resized/train2014' ,\n",
    "                        help='directory for resized training images')\n",
    "    parser.add_argument('--val_dir', type=str, default='./data/resized/val2014',\n",
    "                        help='directory for resized validation images' )\n",
    "    parser.add_argument('--caption_path', type=str,\n",
    "                        default='./data/annotations/captions_train2014.json',\n",
    "                        help='path for train annotation json file')\n",
    "    parser.add_argument('--caption_val_path', type=str,\n",
    "                        default='./data/annotations/captions_val2014.json',\n",
    "                        help='path for validation annotation json file')\n",
    "    parser.add_argument('--log_step', type=int, default=10,\n",
    "                        help='step size for printing log info')\n",
    "    parser.add_argument('--seed', type=int, default=123,\n",
    "                        help='random seed for model reproduction')\n",
    "    \n",
    "    # ---------------------------Hyper Parameter Setup------------------------------------\n",
    "    \n",
    "    # CNN fine-tuning\n",
    "    parser.add_argument('--fine_tune_start_layer', type=int, default=6,\n",
    "                        help='CNN fine-tuning layers from: [0-7]')\n",
    "    parser.add_argument('--cnn_epoch', type=int, default=20,\n",
    "                        help='start fine-tuning CNN after')\n",
    "    \n",
    "    # Optimizer Adam parameter\n",
    "    parser.add_argument( '--alpha', type=float, default=0.8,\n",
    "                         help='alpha in Adam' )\n",
    "    parser.add_argument( '--beta', type=float, default=0.999,\n",
    "                         help='beta in Adam' )\n",
    "    parser.add_argument( '--learning_rate', type=float, default=4e-4,\n",
    "                         help='learning rate for the whole model' )\n",
    "    parser.add_argument( '--learning_rate_cnn', type=float, default=1e-4,\n",
    "                         help='learning rate for fine-tuning CNN' )\n",
    "    \n",
    "    # LSTM hyper parameters\n",
    "    parser.add_argument( '--embed_size', type=int, default=256,\n",
    "                         help='dimension of word embedding vectors, also dimension of v_g' )\n",
    "    parser.add_argument( '--hidden_size', type=int, default=512,\n",
    "                         help='dimension of lstm hidden states' )\n",
    "    \n",
    "    # Training details\n",
    "    parser.add_argument( '--pretrained', type=str, default='', help='start from checkpoint or scratch' )\n",
    "    parser.add_argument( '--num_epochs', type=int, default=50 )\n",
    "    parser.add_argument( '--batch_size', type=int, default=60 )\n",
    "    \n",
    "    # For eval_size > 30, it will cause cuda OOM error.\n",
    "    parser.add_argument( '--eval_size', type=int, default=30 ) \n",
    "    parser.add_argument( '--num_workers', type=int, default=2 )\n",
    "    parser.add_argument( '--clip', type=float, default=0.1 )\n",
    "    parser.add_argument( '--lr_decay', type=int, default=20, help='epoch at which to start lr decay' )\n",
    "    parser.add_argument( '--learning_rate_decay_every', type=int, default=50,\n",
    "                         help='decay learning rate at every this number')\n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print ('------------------------Model and Training Details--------------------------')\n",
    "    print(args)\n",
    "    \n",
    "    # Start training\n",
    "    main( args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
