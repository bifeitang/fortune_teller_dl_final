{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveCNN( nn.Module ):\n",
    "    def __init__( self, embed_size, hidden_size ):\n",
    "        super( AttentiveCNN, self ).__init__()\n",
    "        \n",
    "        \"\"\"# vgg16 backend\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        modules = list( vgg.children() )[ :-1 ] # delete the last fc layer\n",
    "        vgg_conv = nn.Sequential( *modules ) # last conv feature\"\"\"\n",
    "        \n",
    "        # resnet backend\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list( resnet.children() )[ :-2 ] # delete the last fc layer and pooling layer\n",
    "        resnet_conv = nn.Sequential( *modules ) # last conv feature\n",
    "        \n",
    "        \n",
    "        self.resnet_conv = resnet_conv\n",
    "        self.avgpool = nn.AvgPool2d( 7 )\n",
    "        self.affine_a = nn.Linear( 2048, hidden_size ) # v_i = W_a * A\n",
    "        self.affine_b = nn.Linear( 2048, embed_size )  # v_g = W_b * a^g\n",
    "        self.batch_norm = nn.BatchNorm2d(2048, affine=False)\n",
    "        \n",
    "        # Dropout before affine transformation\n",
    "        self.dropout = nn.Dropout( 0.5 )\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights( self ):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        init.kaiming_uniform( self.affine_a.weight, mode='fan_in' )\n",
    "        init.kaiming_uniform( self.affine_b.weight, mode='fan_in' )\n",
    "        self.affine_a.bias.data.fill_( 0 )\n",
    "        self.affine_b.bias.data.fill_( 0 )\n",
    "        \n",
    "        \n",
    "    def forward( self, images ):\n",
    "        '''\n",
    "        Input: images\n",
    "        Output: V=[v_1, ..., v_n], v_g\n",
    "        '''\n",
    "        \n",
    "        # Last conv layer feature map\n",
    "        A = self.resnet_conv( images )\n",
    "        A = self.batch_norm(A)\n",
    "        \n",
    "        # a^g, average pooling feature map\n",
    "        a_g = self.avgpool( A )\n",
    "        a_g = a_g.view( a_g.size(0), -1 )\n",
    "        \n",
    "        # V = [ v_1, v_2, ..., v_49 ]\n",
    "        V = A.view( A.size( 0 ), A.size( 1 ), -1 ).transpose( 1,2 )\n",
    "        V = F.relu( self.affine_a( self.dropout( V ) ) )\n",
    "        \n",
    "        v_g = F.relu( self.affine_b( self.dropout( a_g ) ) )\n",
    "        \n",
    "        return V, v_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Block for C_hat calculation\n",
    "class Atten( nn.Module ):\n",
    "    def __init__( self, hidden_size ):\n",
    "        super( Atten, self ).__init__()\n",
    "\n",
    "        self.affine_v = nn.Linear( hidden_size, 49, bias=False ) # W_v\n",
    "        self.affine_g = nn.Linear( hidden_size, 49, bias=False ) # W_g\n",
    "        self.affine_s = nn.Linear( hidden_size, 49, bias=False ) # W_s\n",
    "        self.affine_h = nn.Linear( 49, 1, bias=False ) # w_h\n",
    "        \n",
    "        self.dropout = nn.Dropout( 0.5 )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights( self ):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        init.xavier_uniform( self.affine_v.weight )\n",
    "        init.xavier_uniform( self.affine_g.weight )\n",
    "        init.xavier_uniform( self.affine_h.weight )\n",
    "        init.xavier_uniform( self.affine_s.weight )\n",
    "        \n",
    "    def forward( self, V, h_t, s_t ):\n",
    "        '''\n",
    "        Input: V=[v_1, v_2, ... v_k], h_t, s_t from LSTM\n",
    "        Output: c_hat_t, attention feature map\n",
    "        '''\n",
    "        \n",
    "        # W_v * V + W_g * h_t * 1^T\n",
    "        content_v = self.affine_v( self.dropout( V ) ).unsqueeze( 1 ) \\\n",
    "                    + self.affine_g( self.dropout( h_t ) ).unsqueeze( 2 )\n",
    "        \n",
    "        # z_t = W_h * tanh( content_v )\n",
    "        z_t = self.affine_h( self.dropout( F.tanh( content_v ) ) ).squeeze( 3 )\n",
    "        alpha_t = F.softmax( z_t.view( -1, z_t.size( 2 ) ) ).view( z_t.size( 0 ), z_t.size( 1 ), -1 )\n",
    "        \n",
    "        # Construct c_t: B x seq x hidden_size\n",
    "        c_t = torch.bmm( alpha_t, V ).squeeze( 2 )\n",
    "        \n",
    "        # W_s * s_t + W_g * h_t\n",
    "        content_s = self.affine_s( self.dropout( s_t ) ) + self.affine_g( self.dropout( h_t ) )\n",
    "        # w_t * tanh( content_s )\n",
    "        z_t_extended = self.affine_h( self.dropout( F.tanh( content_s ) ) )\n",
    "        \n",
    "        # Attention score between sentinel and image content\n",
    "        extended = torch.cat( ( z_t, z_t_extended ), dim=2 )\n",
    "        alpha_hat_t = F.softmax( extended.view( -1, extended.size( 2 ) ) ).view( extended.size( 0 ), extended.size( 1 ), -1 )\n",
    "        beta_t = alpha_hat_t[ :, :, -1 ]\n",
    "        \n",
    "        # c_hat_t = beta * s_t + ( 1 - beta ) * c_t\n",
    "        beta_t = beta_t.unsqueeze( 2 )\n",
    "        c_hat_t = beta_t * s_t + ( 1 - beta_t ) * c_t\n",
    "\n",
    "        return c_hat_t, alpha_t, beta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel BLock    \n",
    "class Sentinel( nn.Module ):\n",
    "    def __init__( self, input_size, hidden_size ):\n",
    "        super( Sentinel, self ).__init__()\n",
    "\n",
    "        self.affine_x = nn.Linear( input_size, hidden_size, bias=False )\n",
    "        self.affine_h = nn.Linear( hidden_size, hidden_size, bias=False )\n",
    "        \n",
    "        # Dropout applied before affine transformation\n",
    "        self.dropout = nn.Dropout( 0.5 )\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights( self ):\n",
    "        init.xavier_uniform( self.affine_x.weight )\n",
    "        init.xavier_uniform( self.affine_h.weight )\n",
    "        \n",
    "    def forward( self, x_t, h_t_1, cell_t ):\n",
    "        \n",
    "        # g_t = sigmoid( W_x * x_t + W_h * h_(t-1) )        \n",
    "        gate_t = self.affine_x( self.dropout( x_t ) ) + self.affine_h( self.dropout( h_t_1 ) )\n",
    "        gate_t = F.sigmoid( gate_t )\n",
    "        \n",
    "        # Sentinel embedding\n",
    "        s_t =  gate_t * F.tanh( cell_t )\n",
    "        \n",
    "        return s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Attention Block: C_t, Spatial Attention Weights, Sentinel embedding    \n",
    "class AdaptiveBlock( nn.Module ):\n",
    "    \n",
    "    def __init__( self, embed_size, hidden_size, vocab_size ):\n",
    "        super( AdaptiveBlock, self ).__init__()\n",
    "\n",
    "        # Sentinel block\n",
    "        self.sentinel = Sentinel( embed_size * 2, hidden_size )\n",
    "        \n",
    "        # Image Spatial Attention Block\n",
    "        self.atten = Atten( hidden_size )\n",
    "        \n",
    "        # Final Caption generator\n",
    "        self.mlp = nn.Linear( hidden_size, vocab_size )\n",
    "        \n",
    "        # Dropout layer inside Affine Transformation\n",
    "        self.dropout = nn.Dropout( 0.5 )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights( self ):\n",
    "        '''\n",
    "        Initialize final classifier weights\n",
    "        '''\n",
    "        init.kaiming_normal( self.mlp.weight, mode='fan_in' )\n",
    "        self.mlp.bias.data.fill_( 0 )\n",
    "        \n",
    "        \n",
    "    def forward( self, x, hiddens, cells, V ):\n",
    "        \n",
    "        # hidden for sentinel should be h0-ht-1\n",
    "        h0 = self.init_hidden( x.size(0) )[0].transpose( 0,1 )\n",
    "        \n",
    "        # h_(t-1): B x seq x hidden_size ( 0 - t-1 )\n",
    "        if hiddens.size( 1 ) > 1:\n",
    "            hiddens_t_1 = torch.cat( ( h0, hiddens[ :, :-1, : ] ), dim=1 )\n",
    "        else:\n",
    "            hiddens_t_1 = h0\n",
    "\n",
    "        # Get Sentinel embedding, it's calculated blockly    \n",
    "        sentinel = self.sentinel( x, hiddens_t_1, cells )\n",
    "        \n",
    "        # Get C_t, Spatial attention, sentinel score\n",
    "        c_hat, atten_weights, beta = self.atten( V, hiddens, sentinel )\n",
    "        \n",
    "        # Final score along vocabulary\n",
    "        scores = self.mlp( self.dropout( c_hat + hiddens ) )\n",
    "        \n",
    "        return scores, atten_weights, beta\n",
    "    \n",
    "    def init_hidden( self, bsz ):\n",
    "        '''\n",
    "        Hidden_0 & Cell_0 initialization\n",
    "        '''\n",
    "        weight = next( self.parameters() ).data\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_().cuda() ),\n",
    "                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_().cuda() ) ) \n",
    "        else: \n",
    "            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_() ),\n",
    "                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_() ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Decoder\n",
    "class Decoder( nn.Module ):\n",
    "    def __init__( self, embed_size, vocab_size, hidden_size ):\n",
    "        super( Decoder, self ).__init__()\n",
    "\n",
    "        # word embedding\n",
    "        self.embed = nn.Embedding( vocab_size, embed_size )\n",
    "        \n",
    "        # LSTM decoder: input = [ w_t; v_g ] => 2 x word_embed_size;\n",
    "        self.LSTM = nn.LSTM( embed_size * 2, hidden_size, 1, batch_first=True )\n",
    "        \n",
    "        # Save hidden_size for hidden and cell variable \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Adaptive Attention Block: Sentinel + C_hat + Final scores for caption sampling\n",
    "        self.adaptive = AdaptiveBlock( embed_size, hidden_size, vocab_size )\n",
    "        \n",
    "    def forward( self, V, v_g , captions, states=None ):\n",
    "        \n",
    "        # Word Embedding\n",
    "        embeddings = self.embed( captions )\n",
    "        \n",
    "        # x_t = [w_t;v_g]\n",
    "        x = torch.cat( ( embeddings, v_g.unsqueeze( 1 ).expand_as( embeddings ) ), dim=2 )\n",
    "        \n",
    "        # Hiddens: Batch x seq_len x hidden_size\n",
    "        # Cells: seq_len x Batch x hidden_size, default setup by Pytorch\n",
    "        if torch.cuda.is_available():\n",
    "            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ).cuda() )\n",
    "            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ).cuda() )\n",
    "        else:\n",
    "            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ) )\n",
    "            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ) )            \n",
    "        \n",
    "        # Recurrent Block\n",
    "        # Retrieve hidden & cell for Sentinel simulation\n",
    "        for time_step in range( x.size( 1 ) ):\n",
    "            \n",
    "            # Feed in x_t one at a time\n",
    "            x_t = x[ :, time_step, : ]\n",
    "            x_t = x_t.unsqueeze( 1 )\n",
    "            \n",
    "            h_t, states = self.LSTM( x_t, states )\n",
    "            \n",
    "            # Save hidden and cell\n",
    "            hiddens[ :, time_step, : ] = h_t  # Batch_first\n",
    "            cells[ time_step, :, : ] = states[ 1 ]\n",
    "        \n",
    "        # cell: Batch x seq_len x hidden_size\n",
    "        cells = cells.transpose( 0, 1 )\n",
    "\n",
    "        # Data parallelism for adaptive attention block\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            ids = range( torch.cuda.device_count() )\n",
    "            adaptive_block_parallel = nn.DataParallel( self.adaptive, device_ids=ids )\n",
    "            \n",
    "            scores, atten_weights, beta = adaptive_block_parallel( x, hiddens, cells, V )\n",
    "        else:\n",
    "            scores, atten_weights, beta = self.adaptive( x, hiddens, cells, V )\n",
    "        \n",
    "        # Return states for Caption Sampling purpose\n",
    "        return scores, states, atten_weights, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Architecture with Image Encoder and Caption decoder        \n",
    "class Encoder2Decoder( nn.Module ):\n",
    "    def __init__( self, embed_size, vocab_size, hidden_size ):\n",
    "        super( Encoder2Decoder, self ).__init__()\n",
    "        \n",
    "        # Image CNN encoder and Adaptive Attention Decoder\n",
    "        self.encoder = AttentiveCNN( embed_size, hidden_size )\n",
    "        self.decoder = Decoder( embed_size, vocab_size, hidden_size )\n",
    "        \n",
    "        \n",
    "    def forward( self, images, captions, lengths ):\n",
    "        \n",
    "        # Data parallelism for V v_g encoder if multiple GPUs are available\n",
    "        # V=[ v_1, ..., v_k ], v_g in the original paper\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_ids = range( torch.cuda.device_count() )\n",
    "            encoder_parallel = torch.nn.DataParallel( self.encoder, device_ids=device_ids )\n",
    "            V, v_g = encoder_parallel( images ) \n",
    "        else:\n",
    "            V, v_g = self.encoder( images )\n",
    "        \n",
    "        # Language Modeling on word prediction\n",
    "        scores, _, _,_ = self.decoder( V, v_g, captions )\n",
    "        \n",
    "        # Pack it to make criterion calculation more efficient\n",
    "        packed_scores = pack_padded_sequence( scores, lengths, batch_first=True )\n",
    "        \n",
    "        return packed_scores\n",
    "    \n",
    "    # Caption generator\n",
    "    def sampler( self, images, max_len=20 ):\n",
    "        \"\"\"\n",
    "        Samples captions for given image features (Greedy search).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Data parallelism if multiple GPUs\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_ids = range( torch.cuda.device_count() )\n",
    "            encoder_parallel = torch.nn.DataParallel( self.encoder, device_ids=device_ids )\n",
    "            V, v_g = encoder_parallel( images ) \n",
    "        else:    \n",
    "            V, v_g = self.encoder( images )\n",
    "            \n",
    "        # Build the starting token Variable <start> (index 1): B x 1\n",
    "        if torch.cuda.is_available():\n",
    "            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ).cuda() )\n",
    "        else:\n",
    "            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ) )\n",
    "        \n",
    "        # Get generated caption idx list, attention weights and sentinel score\n",
    "        sampled_ids = []\n",
    "        attention = []\n",
    "        Beta = []\n",
    "        \n",
    "        # Initial hidden states\n",
    "        states = None\n",
    "\n",
    "        for i in range( max_len ):\n",
    "\n",
    "            scores, states, atten_weights, beta = self.decoder( V, v_g, captions, states ) \n",
    "            predicted = scores.max( 2 )[ 1 ] # argmax\n",
    "            captions = predicted\n",
    "            \n",
    "            # Save sampled word, attention map and sentinel at each timestep\n",
    "            sampled_ids.append( captions )\n",
    "            attention.append( atten_weights )\n",
    "            Beta.append( beta )\n",
    "        \n",
    "        # caption: B x max_len\n",
    "        # attention: B x max_len x 49\n",
    "        # sentinel: B x max_len\n",
    "        sampled_ids = torch.cat( sampled_ids, dim=1 )\n",
    "        attention = torch.cat( attention, dim=1 )\n",
    "        Beta = torch.cat( Beta, dim=1 )\n",
    "        \n",
    "        return sampled_ids, attention, Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from utils import CocoImageFolder, coco_eval, to_var\n",
    "from data_loader import get_loader \n",
    "from build_vocab import Vocabulary\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Model and Training Details--------------------------\n",
      "Namespace(alpha=0.8, batch_size=10, beta=0.999, caption_path='./data/annotations/captions_train2017.json', caption_val_path='./data/annotations/captions_val2017.json', clip=0.1, cnn_epoch=20, crop_size=224, embed_size=256, eval_size=30, f='/run/user/1003/jupyter/kernel-4fba79ec-a942-416f-9081-85d2ff07fad1.json', fine_tune_start_layer=6, hidden_size=512, image_dir='./data/resize/train2017', learning_rate=0.0004, learning_rate_cnn=0.0001, learning_rate_decay_every=50, log_step=50, lr_decay=20, model_path='./models-attentive/', num_epochs=10, num_workers=2, pretrained='', seed=123, val_dir='./data/resize/val2017', vocab_path='./data/vocab.pkl')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Add attributes.')\n",
    "    parser.add_argument( '-f', default='self', help='To make it runnable in jupyter' )\n",
    "    parser.add_argument( '--model_path', type=str, default='./models-attentive/',\n",
    "                         help='path for saving trained models')\n",
    "    parser.add_argument('--crop_size', type=int, default=224 ,\n",
    "                        help='size for randomly cropping images')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl',\n",
    "                        help='path for vocabulary wrapper')\n",
    "    parser.add_argument('--image_dir', type=str, default='./data/resize/train2017' ,\n",
    "                        help='directory for resized training images')\n",
    "    parser.add_argument('--val_dir', type=str, default='./data/resize/val2017',\n",
    "                        help='directory for resized validation images' )\n",
    "    parser.add_argument('--caption_path', type=str,\n",
    "                        default='./data/annotations/captions_train2017.json',\n",
    "                        help='path for train annotation json file')\n",
    "    parser.add_argument('--caption_val_path', type=str,\n",
    "                        default='./data/annotations/captions_val2017.json',\n",
    "                        help='path for validation annotation json file')\n",
    "    parser.add_argument('--log_step', type=int, default=50,\n",
    "                        help='step size for printing log info')\n",
    "    parser.add_argument('--seed', type=int, default=123,\n",
    "                        help='random seed for model reproduction')\n",
    "    \n",
    "    # ---------------------------Hyper Parameter Setup------------------------------------\n",
    "    \n",
    "    # CNN fine-tuning\n",
    "    parser.add_argument('--fine_tune_start_layer', type=int, default=6,\n",
    "                        help='CNN fine-tuning layers from: [0-7]')\n",
    "    parser.add_argument('--cnn_epoch', type=int, default=20,\n",
    "                        help='start fine-tuning CNN after')\n",
    "    \n",
    "    # Optimizer Adam parameter\n",
    "    parser.add_argument( '--alpha', type=float, default=0.8,\n",
    "                         help='alpha in Adam' )\n",
    "    parser.add_argument( '--beta', type=float, default=0.999,\n",
    "                         help='beta in Adam' )\n",
    "    parser.add_argument( '--learning_rate', type=float, default=4e-4,\n",
    "                         help='learning rate for the whole model' )\n",
    "    parser.add_argument( '--learning_rate_cnn', type=float, default=1e-4,\n",
    "                         help='learning rate for fine-tuning CNN' )\n",
    "    \n",
    "    # LSTM hyper parameters\n",
    "    parser.add_argument( '--embed_size', type=int, default=256,\n",
    "                         help='dimension of word embedding vectors, also dimension of v_g' )\n",
    "    parser.add_argument( '--hidden_size', type=int, default=512,\n",
    "                         help='dimension of lstm hidden states' )\n",
    "    \n",
    "    # Training details\n",
    "    parser.add_argument( '--pretrained', type=str, default='', help='start from checkpoint or scratch' )\n",
    "    parser.add_argument( '--num_epochs', type=int, default=10 )\n",
    "    parser.add_argument( '--batch_size', type=int, default=10 )\n",
    "    \n",
    "    # For eval_size > 30, it will cause cuda OOM error.\n",
    "    parser.add_argument( '--eval_size', type=int, default=30 ) \n",
    "    parser.add_argument( '--num_workers', type=int, default=2 )\n",
    "    parser.add_argument( '--clip', type=float, default=0.1 )\n",
    "    parser.add_argument( '--lr_decay', type=int, default=20, help='epoch at which to start lr decay' )\n",
    "    parser.add_argument( '--learning_rate_decay_every', type=int, default=50,\n",
    "                         help='decay learning rate at every this number')\n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print ('------------------------Model and Training Details--------------------------')\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "        transforms.RandomCrop( args.crop_size ),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(( 0.485, 0.456, 0.406 ), \n",
    "                             ( 0.229, 0.224, 0.225 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary wrapper.\n",
    "with open( args.vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load( f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Build training data loader\n",
    "\"\"\"\n",
    "data_loader = get_loader( args.image_dir, args.caption_path, vocab, \n",
    "                          transform, 30,\n",
    "                          shuffle=True, num_workers=args.num_workers )\n",
    "\n",
    "# for testing the data, we can just use validation set to train, use the following loader\n",
    "\"\"\"\n",
    "data_loader = get_loader( args.val_dir, args.caption_val_path, vocab, \n",
    "                          transform, 80,\n",
    "                          shuffle=True, num_workers=args.num_workers ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model or build from scratch\n",
    "adaptive = Encoder2Decoder( args.embed_size, len(vocab), args.hidden_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 3\n",
    "# Constructing CNN parameters for optimization, only fine-tuning higher layers\n",
    "cnn_subs = list( adaptive.encoder.resnet_conv.children() )\n",
    "cnn_params = [ list( sub_module.parameters() ) for sub_module in cnn_subs ]\n",
    "cnn_params = [ item for sublist in cnn_params for item in sublist ]\n",
    "\n",
    "'''cnn_optimizer = torch.optim.Adam( cnn_params, lr=args.learning_rate_cnn, \n",
    "                                  betas=( args.alpha, args.beta ) )'''\n",
    "\n",
    "\n",
    "cnn_optimizer = torch.optim.SGD(cnn_params, lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameter optimization\n",
    "params = list( adaptive.encoder.affine_a.parameters() ) + list( adaptive.encoder.affine_b.parameters() ) \\\n",
    "            + list( adaptive.decoder.parameters() )\n",
    "\n",
    "# Will decay later    \n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "# Language Modeling Loss, Optimizers\n",
    "LMcriterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Change to GPU mode if available\n",
    "if torch.cuda.is_available():\n",
    "    adaptive.cuda()\n",
    "    LMcriterion.cuda()\n",
    "\n",
    "# Train the Models\n",
    "total_step = len( data_loader )\n",
    "\n",
    "bleu_scores = []\n",
    "best_cider = 0.0\n",
    "best_epoch = 0\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Training for Epoch 3----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [0/313], CrossEntropy Loss: 6.2430, Perplexity: 514.3945\n",
      "Epoch [3/10], Step [50/313], CrossEntropy Loss: 5.1451, Perplexity: 171.5811\n",
      "Epoch [3/10], Step [100/313], CrossEntropy Loss: 4.5343, Perplexity: 93.1592\n",
      "Epoch [3/10], Step [150/313], CrossEntropy Loss: 4.2974, Perplexity: 73.5102\n",
      "Epoch [3/10], Step [200/313], CrossEntropy Loss: 4.1376, Perplexity: 62.6515\n",
      "Epoch [3/10], Step [250/313], CrossEntropy Loss: 4.2928, Perplexity: 73.1723\n",
      "Epoch [3/10], Step [300/313], CrossEntropy Loss: 4.1212, Perplexity: 61.6315\n",
      "---------------------Start evaluation on MS-COCO dataset-----------------------\n",
      "[100/167]\n",
      "------------------------Caption Generated-------------------------------------\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47554, 'reflen': 47969, 'guess': [47554, 42554, 37554, 32554], 'correct': [27698, 11236, 3615, 1081]}\n",
      "ratio: 0.9913485792907714\n",
      "Bleu_1: 0.577\n",
      "Bleu_2: 0.389\n",
      "Bleu_3: 0.243\n",
      "Bleu_4: 0.148\n",
      "-----------Evaluation performance on MS-COCO validation dataset for Epoch 3----------\n",
      "Bleu_4: 0.1476\n",
      "Bleu_2: 0.3888\n",
      "Bleu_1: 0.5774\n",
      "Bleu_3: 0.2434\n",
      "------------------Training for Epoch 4----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [0/313], CrossEntropy Loss: 4.0842, Perplexity: 59.3922\n",
      "Epoch [4/10], Step [50/313], CrossEntropy Loss: 3.7888, Perplexity: 44.2020\n",
      "Epoch [4/10], Step [100/313], CrossEntropy Loss: 4.0337, Perplexity: 56.4684\n",
      "Epoch [4/10], Step [150/313], CrossEntropy Loss: 3.9233, Perplexity: 50.5679\n",
      "Epoch [4/10], Step [200/313], CrossEntropy Loss: 3.5814, Perplexity: 35.9254\n",
      "Epoch [4/10], Step [250/313], CrossEntropy Loss: 3.8574, Perplexity: 47.3430\n",
      "Epoch [4/10], Step [300/313], CrossEntropy Loss: 3.5843, Perplexity: 36.0297\n",
      "---------------------Start evaluation on MS-COCO dataset-----------------------\n",
      "[100/167]\n",
      "------------------------Caption Generated-------------------------------------\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 43232, 'reflen': 45234, 'guess': [43232, 38232, 33232, 28232], 'correct': [27955, 12483, 4740, 1580]}\n",
      "ratio: 0.9557412565768901\n",
      "Bleu_1: 0.617\n",
      "Bleu_2: 0.439\n",
      "Bleu_3: 0.297\n",
      "Bleu_4: 0.193\n",
      "-----------Evaluation performance on MS-COCO validation dataset for Epoch 4----------\n",
      "Bleu_4: 0.1934\n",
      "Bleu_2: 0.4387\n",
      "Bleu_1: 0.6174\n",
      "Bleu_3: 0.2970\n",
      "------------------Training for Epoch 5----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/kydn8237872/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [0/313], CrossEntropy Loss: 3.6402, Perplexity: 38.1008\n",
      "Epoch [5/10], Step [50/313], CrossEntropy Loss: 3.4401, Perplexity: 31.1899\n",
      "Epoch [5/10], Step [100/313], CrossEntropy Loss: 3.6523, Perplexity: 38.5614\n",
      "Epoch [5/10], Step [150/313], CrossEntropy Loss: 3.2954, Perplexity: 26.9872\n",
      "Epoch [5/10], Step [200/313], CrossEntropy Loss: 3.5373, Perplexity: 34.3745\n"
     ]
    }
   ],
   "source": [
    "# Start Training \n",
    "for epoch in range(start_epoch, 11):\n",
    "    optimizer = torch.optim.SGD( params, lr=5e-2, momentum=0.9 )\n",
    "    # optimizer = torch.optim.Adam( params, lr=learning_rate )\n",
    "\n",
    "    # Language Modeling Training\n",
    "    print ('------------------Training for Epoch %d----------------'%(epoch))\n",
    "    for i, (images, captions, lengths, _ ) in enumerate( data_loader ):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = to_var( images )\n",
    "        captions = to_var( captions )\n",
    "        lengths = [ cap_len - 1  for cap_len in lengths ]\n",
    "        targets = pack_padded_sequence( captions[:,1:], lengths, batch_first=True )[0]\n",
    "\n",
    "        # Forward, Backward and Optimize\n",
    "        adaptive.train()\n",
    "        adaptive.zero_grad()\n",
    "\n",
    "        packed_scores = adaptive( images, captions, lengths )\n",
    "\n",
    "        # Compute loss and backprop\n",
    "        loss = LMcriterion( packed_scores[0], targets )\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for gradient exploding problem in LSTM\n",
    "        for p in adaptive.decoder.LSTM.parameters():\n",
    "            p.data.clamp_( -args.clip, args.clip )\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Start learning rate decay\n",
    "        if epoch > args.lr_decay:\n",
    "\n",
    "            frac = ( epoch - args.cnn_epoch ) / args.learning_rate_decay_every\n",
    "            decay_factor = math.pow( 0.5, frac )\n",
    "\n",
    "            # Decay the learning rate\n",
    "            learning_rate = learning_rate * decay_factor\n",
    "\n",
    "        # Start CNN fine-tuning\n",
    "        if epoch > 10:\n",
    "            cnn_optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % args.log_step == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], CrossEntropy Loss: %.4f, Perplexity: %5.4f'\\\n",
    "                  %( epoch, args.num_epochs, i, total_step, loss.data[0], np.exp( loss.data[0] ) )) \n",
    "            loss_list.append(loss.data[0])\n",
    "        if i % 500 == 0 and i != 0:\n",
    "            torch.save( adaptive.state_dict(), os.path.join( args.model_path, 'adaptive-%d-%d.pkl'%( epoch, i ) ) )\n",
    "            bleu = coco_eval( adaptive, args, epoch, i )\n",
    "            bleu_scores.append( bleu )\n",
    "            print('Bleu_2 score: %.4f'%(bleu))\n",
    "\n",
    "    # Save the Adaptive Attention model after each epoch\n",
    "    torch.save( adaptive.state_dict(), \n",
    "               os.path.join( args.model_path, 'adaptive-%d-%d.pkl'%( epoch, i ) ) )\n",
    "\n",
    "    # Evaluation on validation set        \n",
    "    bleu = coco_eval( adaptive, args, epoch, i )\n",
    "    bleu_scores.append( bleu )        \n",
    "'''\n",
    "    if cider > best_cider:\n",
    "        best_cider = cider\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if len( cider_scores ) > 5:\n",
    "\n",
    "        last_6 = cider_scores[-6:]\n",
    "        last_6_max = max( last_6 )\n",
    "\n",
    "        # Test if there is improvement, if not do early stopping\n",
    "        if last_6_max != best_cider:\n",
    "\n",
    "            print ('No improvement with CIDEr in the last 6 epochs...Early stopping triggered.')\n",
    "            print ('Model of best epoch #: %d with CIDEr score %.2f'%( best_epoch, best_cider ))\n",
    "            break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
